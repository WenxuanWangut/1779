name: cd
on:
  push:
    branches: [ main ]
  workflow_dispatch:

permissions:
  contents: read
  packages: write
  id-token: write

concurrency:
  group: cd-${{ github.ref }}
  cancel-in-progress: true

env:
  IMAGE_BACKEND: ghcr.io/${{ github.repository_owner }}/app-backend
  IMAGE_FRONTEND: ghcr.io/${{ github.repository_owner }}/app-frontend

jobs:
  detect:
    runs-on: ubuntu-latest
    outputs:
      backend: ${{ steps.filter.outputs.backend }}
      frontend: ${{ steps.filter.outputs.frontend }}
    steps:
      - uses: actions/checkout@v4
      - uses: dorny/paths-filter@v3
        id: filter
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          filters: |
            backend:
              - 'app_backend/**'
              - '.github/workflows/**'
              - 'k8s/**'
            frontend:
              - 'frontend/**'
              - '.github/workflows/**'
              - 'k8s/**'

  build-push-backend:
    needs: detect
    if: needs.detect.outputs.backend == 'true'
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: app_backend
    outputs:
      digest: ${{ steps.build.outputs.digest }}
    steps:
      - uses: actions/checkout@v4
      - uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.IMAGE_BACKEND }}
          tags: |
            type=sha
            type=raw,value=latest,enable=true
      - id: build
        uses: docker/build-push-action@v6
        with:
          context: app_backend/
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}

  build-push-frontend:
    needs: detect
    if: needs.detect.outputs.frontend == 'true'
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: frontend
    outputs:
      digest: ${{ steps.build.outputs.digest }}
    steps:
      - uses: actions/checkout@v4
      - uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.IMAGE_FRONTEND }}
          tags: |
            type=sha
            type=raw,value=latest,enable=true
      - id: build
        uses: docker/build-push-action@v6
        with:
          context: frontend
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          build-args: |
            VITE_API_BASE=/api

  deploy:
    if: ${{ always() }}
    needs: [build-push-backend, build-push-frontend, detect]
    runs-on: ubuntu-latest
    env:
      DO_TOKEN: ${{ secrets.DO_TOKEN }}
      DO_K8S_CLUSTER_ID: ${{ secrets.DO_K8S_CLUSTER_ID }}
      KUBE_CONTEXT: ${{ secrets.KUBE_CONTEXT }}
      KUBE_NAMESPACE: ${{ secrets.KUBE_NAMESPACE }}
    steps:
      - uses: actions/checkout@v4

      - uses: azure/setup-kubectl@v4
        with:
          version: v1.30.0

      - name: Setup doctl
        uses: digitalocean/action-doctl@v2
        with:
          token: ${{ env.DO_TOKEN }}

      - name: Fetch kubeconfig
        run: |
          if [ -z "${DO_K8S_CLUSTER_ID}" ]; then
            echo "Missing DO_K8S_CLUSTER_ID"; exit 1
          fi
          doctl kubernetes cluster kubeconfig save --expiry-seconds 900 "${DO_K8S_CLUSTER_ID}"

      - name: Use context
        if: ${{ env.KUBE_CONTEXT != '' }}
        run: kubectl config use-context "${KUBE_CONTEXT}"

      - name: Ensure namespace exists
        run: |
          NS="${KUBE_NAMESPACE}"
          if [ -z "$NS" ]; then NS=cloudcollab; fi
          echo "KUBE_NS=$NS" >> $GITHUB_ENV
          kubectl get ns "$NS" >/dev/null 2>&1 || kubectl create ns "$NS"

      - name: Ensure jq
        run: sudo apt-get update && sudo apt-get install -y jq

      - name: Patch strategy to Recreate + shorten grace
        run: |
          for DEP in app-backend app-frontend; do
            kubectl -n "$KUBE_NS" patch deploy/$DEP --type='json' -p='[{"op":"remove","path":"/spec/strategy/rollingUpdate"}]' || true
            kubectl -n "$KUBE_NS" patch deploy/$DEP --type='merge' -p='{"spec":{"strategy":{"type":"Recreate"}}}' || true
            kubectl -n "$KUBE_NS" patch deploy/$DEP --type='merge' -p='{"spec":{"template":{"spec":{"terminationGracePeriodSeconds":5}}}}' || true
          done

      - name: Pre-clean backend
        run: |
          kubectl -n "$KUBE_NS" scale deploy/app-backend --replicas=0 || true
          kubectl -n "$KUBE_NS" wait --for=delete pod -l app=app-backend --timeout=120s || true
          PODS=$(kubectl -n "$KUBE_NS" get pod -l app=app-backend -o json | jq -r '.items[]?.metadata.name' || true)
          if [ -n "$PODS" ]; then
            kubectl -n "$KUBE_NS" delete pod $PODS --force --grace-period=0 || true
          fi

      - name: Pre-clean frontend
        run: |
          kubectl -n "$KUBE_NS" scale deploy/app-frontend --replicas=0 || true
          kubectl -n "$KUBE_NS" wait --for=delete pod -l app=app-frontend --timeout=120s || true
          PODS=$(kubectl -n "$KUBE_NS" get pod -l app=app-frontend -o json | jq -r '.items[]?.metadata.name' || true)
          if [ -n "$PODS" ]; then
            kubectl -n "$KUBE_NS" delete pod $PODS --force --grace-period=0 || true
          fi

      - name: Apply kustomize overlay
        run: |
          kubectl kustomize --load-restrictor=LoadRestrictionsNone k8s/overlays/prod | \
          kubectl -n "$KUBE_NS" apply -f -

      - name: Patch services to LoadBalancer
        run: |
          kubectl -n "$KUBE_NS" patch svc app-frontend -p '{"spec":{"type":"LoadBalancer"}}' || true
          kubectl -n "$KUBE_NS" patch svc app-backend -p '{"spec":{"type":"LoadBalancer"}}' || true

      - name: Set images by digest
        run: |
          if [ "${{ needs.detect.outputs.backend }}" = "true" ] && [ -n "${{ needs.build-push-backend.outputs.digest }}" ]; then
            kubectl -n "$KUBE_NS" set image deploy/app-backend app-backend=${IMAGE_BACKEND}@${{ needs.build-push-backend.outputs.digest }} --record=true
          fi
          if [ "${{ needs.detect.outputs.frontend }}" = "true" ] && [ -n "${{ needs.build-push-frontend.outputs.digest }}" ]; then
            kubectl -n "$KUBE_NS" set image deploy/app-frontend app-frontend=${IMAGE_FRONTEND}@${{ needs.build-push-frontend.outputs.digest }} --record=true
          fi

      - name: Scale up both
        run: |
          kubectl -n "$KUBE_NS" scale deploy/app-backend --replicas=1 || true
          kubectl -n "$KUBE_NS" scale deploy/app-frontend --replicas=1 || true

      - name: Rollout backend with rescue
        run: |
          set -e
          rescue() {
            kubectl -n "$KUBE_NS" get pods -l app=app-backend -o wide || true
            kubectl -n "$KUBE_NS" describe deploy/app-backend || true
            PODS=$(kubectl -n "$KUBE_NS" get pod -l app=app-backend -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || true)
            for p in $PODS; do
              kubectl -n "$KUBE_NS" describe pod "$p" || true
              kubectl -n "$KUBE_NS" logs "$p" --tail=200 || true
            done
            TP=$(kubectl -n "$KUBE_NS" get pod -l app=app-backend -o jsonpath='{.items[?(@.metadata.deletionTimestamp)].metadata.name}' 2>/dev/null || true)
            [ -n "$TP" ] && kubectl -n "$KUBE_NS" delete pod $TP --force --grace-period=0 || true
          }
          for i in $(seq 1 10); do
            set +e
            kubectl -n "$KUBE_NS" rollout status deploy/app-backend --timeout=120s
            rc=$?
            set -e
            [ $rc -eq 0 ] && break
            rescue
            sleep 5
          done
          kubectl -n "$KUBE_NS" rollout status deploy/app-backend --timeout=120s

      - name: Rollout frontend with rescue
        run: |
          set -e
          rescue() {
            PODS=$(kubectl -n "$KUBE_NS" get pod -l app=app-frontend -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || true)
            for p in $PODS; do
              kubectl -n "$KUBE_NS" describe pod "$p" || true
              kubectl -n "$KUBE_NS" logs "$p" --tail=200 || true
            done
            TP=$(kubectl -n "$KUBE_NS" get pod -l app=app-frontend -o jsonpath='{.items[?(@.metadata.deletionTimestamp)].metadata.name}' 2>/dev/null || true)
            [ -n "$TP" ] && kubectl -n "$KUBE_NS" delete pod $TP --force --grace-period=0 || true
          }
          for i in $(seq 1 10); do
            set +e
            kubectl -n "$KUBE_NS" rollout status deploy/app-frontend --timeout=120s
            rc=$?
            set -e
            [ $rc -eq 0 ] && break
            rescue
            sleep 5
          done
          kubectl -n "$KUBE_NS" rollout status deploy/app-frontend --timeout=120s

      - name: Wait for LBs ready and print URLs
        id: wait
        run: |
          set -e
          NS="$KUBE_NS"
          for i in $(seq 1 120); do
            FIP=$(kubectl -n "$NS" get svc app-frontend -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || true)
            BIP=$(kubectl -n "$NS" get svc app-backend  -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || true)
            if [ -n "$FIP" ] && [ -n "$BIP" ]; then break; fi
            sleep 5
          done
          echo "FRONTEND_IP=$FIP" >> $GITHUB_OUTPUT
          echo "BACKEND_IP=$BIP" >> $GITHUB_OUTPUT
          echo "Frontend: http://$FIP/"
          echo "Backend:  http://$BIP:8000/"
          for i in $(seq 1 60); do
            EP=$(kubectl -n "$NS" get endpoints app-frontend -o jsonpath='{.subsets[*].addresses[*].ip}' 2>/dev/null || true)
            if [ -n "$EP" ]; then break; fi
            sleep 5
          done
          for i in $(seq 1 60); do
            if curl -fsS --connect-timeout 5 "http://$FIP/healthz" >/dev/null 2>&1; then
              break
            fi
            sleep 5
          done

      - name: Smoke check (frontend)
        if: ${{ steps.wait.outputs.FRONTEND_IP != '' }}
        run: |
          curl -fsS "http://${{ steps.wait.outputs.FRONTEND_IP }}/" | head -c 200